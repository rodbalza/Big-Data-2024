{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del entorno de Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crear la red para que los contenedores de Hadoop, Hive etc puedan comunicarse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker network create bigdata_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Desplegar un clúster de Hadoop en Docker. Utilizaremos imágenes de Docker disponibles para Hadoop y Hive en un clúster básico.\n",
    "- Crea un archivo `docker-compose.yml` con las configuraciones necesarias para un clúster de Hadoop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Yaml\n",
    "version: '3'\n",
    "services:\n",
    "  namenode:\n",
    "    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8\n",
    "    container_name: namenode\n",
    "    environment:\n",
    "      - CLUSTER_NAME=test\n",
    "    ports:\n",
    "      - 9870:9870\n",
    "      - 9000:9000\n",
    "    networks:\n",
    "      - bigdata_network\n",
    "\n",
    "  datanode:\n",
    "    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8\n",
    "    container_name: datanode\n",
    "    environment:\n",
    "      - CLUSTER_NAME=test\n",
    "      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000\n",
    "    networks:\n",
    "      - bigdata_network\n",
    "\n",
    "  hive:\n",
    "    image: bde2020/hive:2.3.2-postgresql-metastore\n",
    "    container_name: hive-server\n",
    "    environment:\n",
    "      - HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://metastore:5432/metastore\n",
    "    ports:\n",
    "      - 10000:10000\n",
    "    networks:\n",
    "      - bigdata_network\n",
    "\n",
    "  metastore:\n",
    "    image: bde2020/hive-metastore-postgresql\n",
    "    container_name: metastore\n",
    "    environment:\n",
    "      - POSTGRES_DB=metastore\n",
    "      - POSTGRES_USER=hive\n",
    "      - POSTGRES_PASSWORD=hive\n",
    "    networks:\n",
    "      - bigdata_network\n",
    "\n",
    "networks:\n",
    "  bigdata_network:\n",
    "    external: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iniciar el clúster de Hadoop y Hive. Ejecuta el siguiente comando en el directorio donde creaste `docker-compose.yml`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este comando desplegará los contenedores necesarios para el clúster de Hadoop y el servicio de Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesta y Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preparar los datos para cargar en Hadoop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que tienes un archivo `sales_data.csv` con datos de ventas que deseas cargar en HDFS.  \n",
    " \n",
    "Mueve el archivo `sales_data.csv` al contenedor namenode para cargarlo en HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker cp sales_data.csv namenode:/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cargar el archivo en HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accede al contenedor `namenode` y carga el archivo en HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker exec -it namenode bash\n",
    "hdfs dfs -mkdir -p /data/sales\n",
    "hdfs dfs -put /sales_data.csv /data/sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto almacena el archivo `sales_data.csv` en el sistema de archivos distribuido HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultas SQL con Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear una tabla en Hive. Accede al contenedor `hive-server` para crear una tabla y cargar los datos en Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker exec -it hive-server bash\n",
    "hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el intérprete de Hive, crea una tabla para los datos de ventas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE IF NOT EXISTS sales_data (\n",
    "    order_id STRING,\n",
    "    product STRING,\n",
    "    quantity INT,\n",
    "    price FLOAT,\n",
    "    order_date STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/data/sales';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este comando configura una tabla que utiliza los datos en HDFS y permite realizar consultas sobre ella."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar una consulta en Hive para agregación. Ejecuta una consulta en Hive para calcular las ventas totales por producto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "SELECT product, SUM(quantity * price) AS total_sales\n",
    "FROM sales_data\n",
    "GROUP BY product;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarda el resultado como una nueva tabla o exporta los datos a HDFS para análisis posterior en Power BI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de Datos en Power BI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conectar Power BI a Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abre Power BI y selecciona Obtener datos > Más.  \n",
    "\n",
    "Escoge ODBC o JDBC como conector (asegúrate de tener el conector de Hive configurado en tu sistema).  \n",
    "\n",
    "Usa la conexión a tu servicio de Hive en Docker (hive-server:10000) para acceder a los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crear Visualizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea visualizaciones, como gráficos de barras, para mostrar las ventas totales por producto.  \n",
    "Usa la opción de actualización automática de Power BI para reflejar cualquier cambio en los datos de Hive.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatización y Escalabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Automatizar el flujo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes configurar Apache Airflow en Docker para automatizar la ingesta, transformación y carga de datos. Usa airflow para programar tareas periódicas (por ejemplo, actualizar datos en Hive y cargar resultados en Power BI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Escalar el entorno con Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si necesitas mayor capacidad de procesamiento, considera desplegar el clúster en Kubernetes.  \n",
    "Usa un archivo yaml de Kubernetes para definir los despliegues y replica tus servicios de Hadoop y Hive en múltiples nodos, facilitando la distribución de carga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
